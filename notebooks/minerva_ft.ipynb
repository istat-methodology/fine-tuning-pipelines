{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Minerva - Fine Tuning for Sequence Classification\n",
        "This notebook explores fine-tuning Minerva models for text classification. We will load the model using `AutoModelForSequenceClassification` specifying the number of classes instead of loading it via `AutoModelForCausalLM`. The former swaps the LLM's generative head for a classification head."
      ],
      "metadata": {
        "id": "6t6nSdo2GuTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0jxC2p7GmWL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from peft import(\n",
        "    LoraConfig,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model\n",
        ")\n",
        "from dotenv import load_dotenv\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configs\n",
        "Here, we can set some parameters for importing and training."
      ],
      "metadata": {
        "id": "N3TQBCbKHE8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_version : str   = '350M'\n",
        "model_id      : str   = f'sapienzanlp/Minerva-{model_version}-base-v1.0'\n",
        "dataset_id    : str   = 'istat-ai/sentipolc_dataset'\n",
        "num_labels    : int   = 3\n",
        "max_model_len : int   = 16384\n",
        "\n",
        "lora_r        : int   = 16\n",
        "lora_alpha    : int   = 8\n",
        "target_modules: list  = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
        "lora_dropout  : float = 0.05\n",
        "lora_bias     : str   = 'none'\n",
        "\n",
        "output_dir    : str   = f'saved_models/Minerva-{model_version}-base-v1.0'\n",
        "epochs        : int   = 3\n",
        "learn_rate    : float = 2e-5\n",
        "scheduler     : str   = 'linear'\n",
        "train_bs      : int   = 16\n",
        "eval_bs       : int   = 32\n",
        "ga_steps      : int   = 2\n",
        "decay         : float = 0.01\n",
        "warmup        : float = 0.1\n",
        "log_steps     : int   = 20\n",
        "eval_strategy : str   = 'steps'\n",
        "save_strategy : str   = 'steps'\n",
        "fp16          : bool  = True\n",
        "load_best     : bool  = True\n",
        "report_to     : list  = []\n",
        "log_level     : str   = 'warning'"
      ],
      "metadata": {
        "id": "MdxYRng0HId4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "VcmA8JbjHjEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model\n",
        "Load the model and tokenizer from huggingface. We will quantize the model to 4-bit precision and prepare it for parameter-efficient fine-tuning (PEFT). If the model is gated or private, you need to set an environment variable called `\"HF_TOKEN\"` that contans your huggingface token."
      ],
      "metadata": {
        "id": "sqkGDyYeH1f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=lora_bias,\n",
        "    task_type='SEQ_CLS'\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=num_labels,\n",
        "    quantization_config=quant_config\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "BR9t3ERCHkBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "Load the data from Hugging Face. The data should have a `text` column and a `label` column that comprises numerical labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "3EHWLYkXIVxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(dataset_id)"
      ],
      "metadata": {
        "id": "DceNcA__IhP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we tokenize and pad the data using the pretrained tokenizer."
      ],
      "metadata": {
        "id": "NM02eEr-IpiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=max_model_len)\n",
        "\n",
        "tokenized_data = data.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "tVAIB3xsIsH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "HCUg_VmTIvLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "First, we define a function to compute the metrics that we want to monitor during training."
      ],
      "metadata": {
        "id": "ZtjaLbPDIwsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='macro')\n",
        "    return {'accuracy': accuracy, 'f1_macro': f1}"
      ],
      "metadata": {
        "id": "COonGoL3Iv0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define the training arguments and trainer classes."
      ],
      "metadata": {
        "id": "pIZLrLpUI5d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=learn_rate,\n",
        "    lr_scheduler_type=scheduler,\n",
        "    per_device_train_batch_size=train_bs,\n",
        "    per_device_eval_batch_size=eval_bs,\n",
        "    gradient_accumulation_steps=ga_steps,\n",
        "    warmup_ratio=warmup,\n",
        "    weight_decay=decay,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=log_steps,\n",
        "    eval_strategy=eval_strategy,\n",
        "    save_strategy=save_strategy,\n",
        "    fp16=fp16,\n",
        "    load_best_model_at_end=load_best,\n",
        "    report_to=report_to,\n",
        "    log_level=log_level,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data['train'],\n",
        "    eval_dataset=tokenized_data['eval'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "gMyB_StEI9Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can start training the model."
      ],
      "metadata": {
        "id": "fw1uoWhwJADn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "e85Yip1dJCRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "We can now evaluate the model on our test set."
      ],
      "metadata": {
        "id": "dloTRUTMJEmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(tokenized_data['test'])\n",
        "eval_results"
      ],
      "metadata": {
        "id": "kxHE6VSiJH3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}