{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-3.2 Fine Tuning\n",
    "This notebook explores fine-tuning Llama-3.2 models for text classification. We will load the model using `AutoModelForSequenceClassification` specifying the number of classes instead of loading it via `AutoModelForCausalLM`. The former swaps the LLM's generative head for a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import(\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "Here, we can set some parameters for importing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version : str   = '1B'\n",
    "model_id      : str   = f'meta-llama/Llama-3.2-{model_version}'\n",
    "dataset_id    : str   = 'istat-ai/hs_dataset'\n",
    "num_labels    : int   = 2\n",
    "quantize_4bit : bool  = True\n",
    "\n",
    "lora_r        : int   = 16\n",
    "lora_alpha    : int   = 8\n",
    "target_modules: list  = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "lora_dropout  : float = 0.05\n",
    "lora_bias     : str   = 'none'\n",
    "\n",
    "output_dir    : str   = f'saved_models/llama-3.2-{model_version}'\n",
    "epochs        : int   = 10\n",
    "learn_rate    : float = 2e-5\n",
    "scheduler     : str   = 'linear'\n",
    "train_bs      : int   = 16\n",
    "eval_bs       : int   = 32\n",
    "ga_steps      : int   = 2\n",
    "decay         : float = 0.01\n",
    "warmup        : float = 0.1\n",
    "log_steps     : int   = 10\n",
    "eval_strategy : str   = 'epoch'\n",
    "save_strategy : str   = 'epoch'\n",
    "fp16          : bool  = True\n",
    "load_best     : bool  = True\n",
    "report_to     : list  = []\n",
    "log_level     : str   = 'warning'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "Load the model and tokenizer from huggingface. We will quantize the model to 4-bit precision and prepare it for parameter-efficient fine-tuning (PEFT). If the model is gated or private, you need to set an environment variable called `\"HF_TOKEN\"` that contans your huggingface token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=lora_bias,\n",
    "    task_type='SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Load the data from huggingface. The data should have a `text` column and a `label` column that comprises numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tokenize and pad the data using the pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=tokenizer.model_max_length)\n",
    "\n",
    "tokenized_data = data.map(\n",
    "    tokenize,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "First, we define a function to compute the metrics that we want to monitor during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': accuracy, 'f1_macro': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the training aruments and trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=learn_rate,\n",
    "    lr_scheduler_type=scheduler,\n",
    "    per_device_train_batch_size=train_bs,\n",
    "    per_device_eval_batch_size=eval_bs,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    warmup_ratio=warmup,\n",
    "    weight_decay=decay,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=log_steps,\n",
    "    eval_strategy=eval_strategy,\n",
    "    save_strategy=save_strategy,\n",
    "    fp16=fp16,\n",
    "    load_best_model_at_end=load_best,\n",
    "    report_to=report_to,\n",
    "    log_level=log_level,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['eval'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now, we can evaluate the model on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate(tokenized_data['test'])\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
